{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8110fc71",
   "metadata": {
    "problem_id": "ex1"
   },
   "source": [
    "#### Exercise 1\n",
    "<!-- @q -->\n",
    "\n",
    "1. What kinds of EDA techniques might you use to explore the following types of data:\n",
    "    - Numeric data?\n",
    "      Descriptive statistics (mean, median, mode, std, min, max, quartiles)\n",
    "      Distribution analysis with histograms and box plots\n",
    "      Density plots and Q-Q plots for normality testing\n",
    "      Correlation analysis with correlation matrices and heatmaps\n",
    "      Scatter plots for relationships between variables\n",
    "      Outlier detection using IQR method or z-scores\n",
    "      \n",
    "    - Categorical data?\n",
    "     Frequency tables and value counts\n",
    "     Bar charts and count plots for visualization\n",
    "     Cross-tabulation for relationships between categorical variables\n",
    "     Pie charts for proportion visualization\n",
    "     Chi-square tests for independence testing\n",
    "  \n",
    "\n",
    "    - The relationship between categorical and numeric data?\n",
    "      Group-by statistics (mean, median by category)\n",
    "      Box plots comparing numeric values across categories\n",
    "      Violin plots for distribution comparison\n",
    "      ANOVA tests for statistical significance\n",
    "      Scatter plots with categorical encoding (colors/shapes)\n",
    "      Pivot tables for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b98cf",
   "metadata": {
    "part_id": "ex1-part1",
    "span": "ex1-part1.answer",
    "student": true
   },
   "source": [
    "*Enter your answer in this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af7624",
   "metadata": {
    "part_id": "ex1-part2"
   },
   "source": [
    "2. Generate some fake data (~1000 rows) with 1 categorical column (with 10 categories) and 2 numeric columns. Use the techniques you mentioned to explore the numeric, categorical, and the relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5caa89c8-8afb-4b6c-8cd1-a24b377854bb",
   "metadata": {
    "additional_cells_expected": true,
    "part_id": "ex1-part2",
    "span": "ex1-part2.code",
    "student": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9bdce91",
   "metadata": {
    "additional_cells_expected": true,
    "part_id": "ex1-part2",
    "span": "ex1-part2.code",
    "student": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1000, 3)\n",
      "\n",
      "First 5 rows:\n",
      "  categorical_feature  numeric_feature_1  numeric_feature_2\n",
      "0          Category_G          63.017736           0.205312\n",
      "1          Category_D          35.952290           3.960813\n",
      "2          Category_H          36.867125           1.232973\n",
      "3          Category_E          53.069153           2.766867\n",
      "4          Category_G          40.960283           0.686636\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Generate fake data with ~1000 rows\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# 1 categorical column with 10 categories\n",
    "categories = ['Category_A', 'Category_B', 'Category_C', 'Category_D', 'Category_E', \n",
    "              'Category_F', 'Category_G', 'Category_H', 'Category_I', 'Category_J']\n",
    "categorical_data = np.random.choice(categories, n_samples)\n",
    "\n",
    "# 2 numeric columns\n",
    "numeric_col1 = np.random.normal(50, 15, n_samples)  # mean=50, std=15\n",
    "numeric_col2 = np.random.exponential(2, n_samples)  # exponential distribution\n",
    "\n",
    "# Create DataFrame\n",
    "df_eda = pd.DataFrame({\n",
    "    'categorical_feature': categorical_data,\n",
    "    'numeric_feature_1': numeric_col1,\n",
    "    'numeric_feature_2': numeric_col2\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df_eda.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_eda.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32a8118",
   "metadata": {
    "additional_cells_expected": true,
    "part_id": "ex1-part2",
    "span": "ex1-part2.code",
    "student": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. EDA FOR NUMERIC DATA:\n",
      "==============================\n",
      "Descriptive Statistics:\n",
      "       numeric_feature_1  numeric_feature_2\n",
      "count        1000.000000        1000.000000\n",
      "mean           50.156663           2.065296\n",
      "std            15.363112           2.094032\n",
      "min            -3.523079           0.000377\n",
      "25%            40.271307           0.585889\n",
      "50%            50.410394           1.478348\n",
      "75%            60.953650           2.861854\n",
      "max            97.129505          16.317668\n",
      "\n",
      "Correlation Matrix:\n",
      "                   numeric_feature_1  numeric_feature_2\n",
      "numeric_feature_1           1.000000           0.008202\n",
      "numeric_feature_2           0.008202           1.000000\n",
      "\n",
      "Outlier Analysis for numeric_feature_1:\n",
      "Number of outliers detected: 6\n",
      "Percentage of outliers: 0.60%\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# EDA for Numeric Data\n",
    "print(\"1. EDA FOR NUMERIC DATA:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Descriptive statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(df_eda[['numeric_feature_1', 'numeric_feature_2']].describe())\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = df_eda[['numeric_feature_1', 'numeric_feature_2']].corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Outlier detection using IQR method for numeric_feature_1\n",
    "Q1 = df_eda['numeric_feature_1'].quantile(0.25)\n",
    "Q3 = df_eda['numeric_feature_1'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df_eda[(df_eda['numeric_feature_1'] < lower_bound) | \n",
    "                  (df_eda['numeric_feature_1'] > upper_bound)]\n",
    "\n",
    "print(f\"\\nOutlier Analysis for numeric_feature_1:\")\n",
    "print(f\"Number of outliers detected: {len(outliers)}\")\n",
    "print(f\"Percentage of outliers: {len(outliers)/len(df_eda)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d6dab1",
   "metadata": {
    "additional_cells_expected": true,
    "part_id": "ex1-part2",
    "span": "ex1-part2.code",
    "student": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. EDA FOR CATEGORICAL DATA:\n",
      "================================\n",
      "Value Counts:\n",
      "categorical_feature\n",
      "Category_A    118\n",
      "Category_C    110\n",
      "Category_E    107\n",
      "Category_J    107\n",
      "Category_H    100\n",
      "Category_F     96\n",
      "Category_G     94\n",
      "Category_D     94\n",
      "Category_I     91\n",
      "Category_B     83\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions:\n",
      "categorical_feature\n",
      "Category_A    0.118\n",
      "Category_C    0.110\n",
      "Category_E    0.107\n",
      "Category_J    0.107\n",
      "Category_H    0.100\n",
      "Category_F    0.096\n",
      "Category_G    0.094\n",
      "Category_D    0.094\n",
      "Category_I    0.091\n",
      "Category_B    0.083\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Number of unique categories: 10\n",
      "Most frequent category: <pandas.core.indexing._iLocIndexer object at 0x12caea3a0>\n",
      "\n",
      "Frequency Table:\n",
      "     Category  Frequency  Percentage\n",
      "0  Category_A        118        11.8\n",
      "1  Category_C        110        11.0\n",
      "2  Category_E        107        10.7\n",
      "3  Category_J        107        10.7\n",
      "4  Category_H        100        10.0\n",
      "5  Category_F         96         9.6\n",
      "6  Category_G         94         9.4\n",
      "7  Category_D         94         9.4\n",
      "8  Category_I         91         9.1\n",
      "9  Category_B         83         8.3\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# EDA for Categorical Data\n",
    "print(\"2. EDA FOR CATEGORICAL DATA:\")\n",
    "print(\"=\"*32)\n",
    "\n",
    "# Frequency tables and value counts\n",
    "print(\"Value Counts:\")\n",
    "print(df_eda['categorical_feature'].value_counts())\n",
    "\n",
    "print(\"\\nProportions:\")\n",
    "proportions = df_eda['categorical_feature'].value_counts(normalize=True)\n",
    "print(proportions)\n",
    "\n",
    "print(f\"\\nNumber of unique categories: {df_eda['categorical_feature'].nunique()}\")\n",
    "print(f\"Most frequent category: {df_eda['categorical_feature'].mode().iloc}\")\n",
    "\n",
    "# Create a frequency table\n",
    "freq_table = pd.DataFrame({\n",
    "    'Category': df_eda['categorical_feature'].value_counts().index,\n",
    "    'Frequency': df_eda['categorical_feature'].value_counts().values,\n",
    "    'Percentage': df_eda['categorical_feature'].value_counts(normalize=True).values * 100\n",
    "})\n",
    "print(\"\\nFrequency Table:\")\n",
    "print(freq_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df5799fe",
   "metadata": {
    "additional_cells_expected": true,
    "part_id": "ex1-part2",
    "span": "ex1-part2.code",
    "student": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. EDA FOR RELATIONSHIP BETWEEN CATEGORICAL AND NUMERIC DATA:\n",
      "=================================================================\n",
      "Group-by Statistics for numeric_feature_1 by categorical_feature:\n",
      "                     count    mean  median     std     min     max\n",
      "categorical_feature                                               \n",
      "Category_A             118  49.656  50.262  14.655  13.173  86.808\n",
      "Category_B              83  53.515  51.664  17.955  12.060  94.193\n",
      "Category_C             110  52.129  51.513  13.914  25.607  87.686\n",
      "Category_D              94  49.804  51.382  15.425  10.291  85.455\n",
      "Category_E             107  50.372  52.235  14.461   7.907  96.411\n",
      "Category_F              96  51.481  50.572  14.816  12.972  89.376\n",
      "Category_G              94  50.471  50.204  14.320   9.313  84.063\n",
      "Category_H             100  48.700  49.129  17.083  -0.826  97.130\n",
      "Category_I              91  46.977  48.772  14.894  15.590  88.461\n",
      "Category_J             107  48.771  48.968  15.998  -3.523  87.021\n",
      "\n",
      "Group-by Statistics for numeric_feature_2 by categorical_feature:\n",
      "                     count   mean  median    std    min     max\n",
      "categorical_feature                                            \n",
      "Category_A             118  1.981   1.368  1.745  0.013   9.253\n",
      "Category_B              83  1.983   1.438  2.009  0.036  11.671\n",
      "Category_C             110  2.014   1.423  1.903  0.006  11.105\n",
      "Category_D              94  1.910   1.206  2.159  0.001  15.386\n",
      "Category_E             107  2.408   1.381  2.780  0.000  16.318\n",
      "Category_F              96  2.183   1.673  2.171  0.020  15.071\n",
      "Category_G              94  1.961   1.293  1.899  0.004   8.024\n",
      "Category_H             100  2.434   1.650  2.403  0.062  10.541\n",
      "Category_I              91  1.745   1.191  1.809  0.005   9.593\n",
      "Category_J             107  1.983   1.492  1.828  0.006   9.228\n",
      "\n",
      "Pivot Table (Mean values):\n",
      "                     numeric_feature_1  numeric_feature_2\n",
      "categorical_feature                                      \n",
      "Category_A                      49.656              1.981\n",
      "Category_B                      53.515              1.983\n",
      "Category_C                      52.129              2.014\n",
      "Category_D                      49.804              1.910\n",
      "Category_E                      50.372              2.408\n",
      "Category_F                      51.481              2.183\n",
      "Category_G                      50.471              1.961\n",
      "Category_H                      48.700              2.434\n",
      "Category_I                      46.977              1.745\n",
      "Category_J                      48.771              1.983\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# EDA for Relationship between Categorical and Numeric Data\n",
    "print(\"3. EDA FOR RELATIONSHIP BETWEEN CATEGORICAL AND NUMERIC DATA:\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Group-by statistics\n",
    "print(\"Group-by Statistics for numeric_feature_1 by categorical_feature:\")\n",
    "group_stats1 = df_eda.groupby('categorical_feature')['numeric_feature_1'].agg([\n",
    "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(3)\n",
    "print(group_stats1)\n",
    "\n",
    "print(\"\\nGroup-by Statistics for numeric_feature_2 by categorical_feature:\")\n",
    "group_stats2 = df_eda.groupby('categorical_feature')['numeric_feature_2'].agg([\n",
    "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(3)\n",
    "print(group_stats2)\n",
    "\n",
    "# Create pivot table\n",
    "print(\"\\nPivot Table (Mean values):\")\n",
    "pivot_table = df_eda.pivot_table(\n",
    "    values=['numeric_feature_1', 'numeric_feature_2'], \n",
    "    index='categorical_feature', \n",
    "    aggfunc='mean'\n",
    ").round(3)\n",
    "print(pivot_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25320be1",
   "metadata": {
    "problem_id": "2"
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "\n",
    "Generate a data set you can use with a supervised ML model.  The data should meet the following criteria:\n",
    "   - It should have 1000 rows\n",
    "   - It should have 6 columns, with one column (your \"target\" column being a boolean column), one categorical column with 5 categories, and 4 numeric columns.\n",
    "   - The numeric columns should have dramatically different scales - different means, different std. deviations.\n",
    "   - Each non-target column should have about 5% nulls.\n",
    "\n",
    "Make this data a little more interesting by calculating the target column using a noisy function of the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a941ebec",
   "metadata": {
    "additional_cells_expected": true,
    "part_id": "2-part1",
    "span": "2-part1.code",
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Generate dataset for supervised ML\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate 4 numeric columns with dramatically different scales\n",
    "numeric_col1 = np.random.normal(10, 2, n_samples)           # Small scale: mean=10, std=2\n",
    "numeric_col2 = np.random.normal(1000, 200, n_samples)      # Medium scale: mean=1000, std=200  \n",
    "numeric_col3 = np.random.normal(0.01, 0.005, n_samples)    # Very small scale: mean=0.01, std=0.005\n",
    "numeric_col4 = np.random.normal(50000, 15000, n_samples)   # Large scale: mean=50000, std=15000\n",
    "\n",
    "# Generate 1 categorical column with 5 categories\n",
    "categories = ['Type_A', 'Type_B', 'Type_C', 'Type_D', 'Type_E']\n",
    "categorical_col = np.random.choice(categories, n_samples)\n",
    "\n",
    "# Create DataFrame with all features\n",
    "df = pd.DataFrame({\n",
    "    'numeric_1': numeric_col1,\n",
    "    'numeric_2': numeric_col2, \n",
    "    'numeric_3': numeric_col3,\n",
    "    'numeric_4': numeric_col4,\n",
    "    'categorical': categorical_col\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d660fce9",
   "metadata": {
    "additional_cells_expected": true,
    "part_id": "2-part1",
    "span": "2-part1.code",
    "student": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n",
      "target\n",
      "True     512\n",
      "False    488\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final dataset shape: (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Add 5% nulls to each non-target column\n",
    "def add_nulls(series, null_percentage=0.05):\n",
    "    n_nulls = int(len(series) * null_percentage)\n",
    "    null_indices = np.random.choice(len(series), n_nulls, replace=False)\n",
    "    series_copy = series.copy()\n",
    "    series_copy.iloc[null_indices] = np.nan\n",
    "    return series_copy\n",
    "\n",
    "df['numeric_1'] = add_nulls(df['numeric_1'])\n",
    "df['numeric_2'] = add_nulls(df['numeric_2'])\n",
    "df['numeric_3'] = add_nulls(df['numeric_3'])\n",
    "df['numeric_4'] = add_nulls(df['numeric_4'])\n",
    "df['categorical'] = add_nulls(df['categorical'])\n",
    "\n",
    "# Create target column using a noisy function of other columns\n",
    "df_no_nulls = df.fillna(df.mean(numeric_only=True))  # Fill numeric nulls with mean\n",
    "df_no_nulls['categorical'] = df_no_nulls['categorical'].fillna('Type_A')  # Fill categorical nulls\n",
    "\n",
    "def create_target(row):\n",
    "    # Normalize features to similar scales for calculation\n",
    "    norm_n1 = (row['numeric_1'] - 10) / 2          \n",
    "    norm_n2 = (row['numeric_2'] - 1000) / 200      \n",
    "    norm_n3 = (row['numeric_3'] - 0.01) / 0.005    \n",
    "    norm_n4 = (row['numeric_4'] - 50000) / 15000   \n",
    "    \n",
    "    # Categorical effect\n",
    "    cat_effect = {'Type_A': 0.2, 'Type_B': -0.1, 'Type_C': 0.0, 'Type_D': 0.3, 'Type_E': -0.2}\n",
    "    cat_val = cat_effect[row['categorical']]\n",
    "    \n",
    "    # Complex function with interactions\n",
    "    score = (0.3 * norm_n1 + \n",
    "             0.2 * norm_n2 + \n",
    "             -0.4 * norm_n3 + \n",
    "             0.1 * norm_n4 + \n",
    "             cat_val +\n",
    "             0.2 * norm_n1 * norm_n2 +  # interaction term\n",
    "             0.1 * norm_n3 * norm_n4)   # another interaction\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 0.5)\n",
    "    score_with_noise = score + noise\n",
    "    \n",
    "    # Convert to boolean (threshold at 0)\n",
    "    return score_with_noise > 0\n",
    "\n",
    "# Apply function to create target\n",
    "np.random.seed(42)  # Reset seed for reproducible noise\n",
    "df['target'] = df_no_nulls.apply(create_target, axis=1)\n",
    "\n",
    "print(f\"Target distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678de6",
   "metadata": {
    "problem_id": "ex2"
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Use whatever resources you need to figure out how to build an SKLearn ML pipelines. Use a pipeline to build an ML approach to predicting your target column in the preceding data with logistic regression.  I have set up the problem below so that you will write your code in a function function call that takes an SKLearn model and data frame and returns the results of a cross validation scoring routine.  \n",
    "\n",
    "I have not taught you how to do this; use the book, google, the notes, chatgpt, or whatever. This is a test of your ability to *find* information, and use this to construct a solution. Your solution should:\n",
    "\n",
    "- Use a transformer pipeline that processes your numeric and categorical features separately\n",
    "- Place everything in a pipeline with the classifier that is passed in to the function.\n",
    "- I've already implemented the call to cross_val_score - to make it work, you'll need to assign your pipeline to the `pipeline` variable.\n",
    "\n",
    "_Note: You could just feed this question to AI and get an answer, and chances are, it will be right. But if you do, you won't really learn much. So, be thoughtful in your use of AI here - you can use it to build the solution step by step, and it will explain how everything works. It's all in how you use it. So, it's your choice - go for the easy grade, or learn something._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4bd23a7",
   "metadata": {
    "part_id": "ex2-part1",
    "span": "ex2-part1.fill",
    "student": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (5-fold): mean=0.948, std=0.018\n",
      "Fold scores: [0.956 0.937 0.956 0.92  0.971]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports (already done above)\n",
    "\n",
    "def run_classifier(df,classifier):\n",
    "    # Separate features/target\n",
    "    y = df[\"target\"].astype(int) # logistic expects numeric; 0/1 from boolean\n",
    "    X = df.drop(columns=[\"target\"])\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Create preprocessing pipelines for numeric and categorical data\n",
    "    \n",
    "    # Numeric preprocessing: impute missing values + scale\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "        ('scaler', StandardScaler())  # Standardize features\n",
    "    ])\n",
    "    \n",
    "    # Categorical preprocessing: impute missing values + one-hot encode\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values\n",
    "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))  # One-hot encode\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Create the complete pipeline with preprocessing + classifier\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # --- 5-fold CV using F1\n",
    "    return cross_val_score(pipeline, X, y, scoring=\"f1\", cv=5)\n",
    "\n",
    "# Test with Logistic Regression\n",
    "scores = run_classifier(df,LogisticRegression(random_state=42))\n",
    "print(f\"F1 (5-fold): mean={scores.mean():.3f}, std={scores.std():.3f}\")\n",
    "print(\"Fold scores:\", np.round(scores, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06eb0b",
   "metadata": {
    "part_id": "ex2-part2"
   },
   "source": [
    "Try using a `RandomForestClassifier` in the preceding pipeline. Just call `run_classifier` with a `RandomForestClassifier`, and print out the results as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a65de74c",
   "metadata": {
    "part_id": "ex2-part2",
    "span": "ex2-part2.code",
    "student": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (5-fold): mean=0.929, std=0.021\n",
      "Fold scores: [0.936 0.906 0.946 0.902 0.956]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Test with Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "scores_rf = run_classifier(df, RandomForestClassifier(random_state=42))\n",
    "print(f\"F1 (5-fold): mean={scores_rf.mean():.3f}, std={scores_rf.std():.3f}\")\n",
    "print(\"Fold scores:\", np.round(scores_rf, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bbb3b1",
   "metadata": {
    "part_id": "ex2-part3"
   },
   "source": [
    "Normally, `RandomForestClassifier`s are considered to be more powerful than `LogisticRegression`.  Depending on your data, this may or may not be the case. Reflect on your answers - which one does better here, and why do you think that is?  Once again, you might use AI, but you should probably also try to _understand_ the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb72f7e",
   "metadata": {
    "part_id": "ex2-part3",
    "span": "ex2-part3.answer",
    "student": true
   },
   "source": [
    "In this specific case, Logistic Regression performed better than Random Forest. Here are the likely reasons:\n",
    "\n",
    "1. Linear Relationships:\n",
    "The target variable was created using a linear combination of normalized features plus noise. Since the underlying relationship is fundamentally linear (with some interaction terms), Logistic Regression is well-suited to capture this pattern directly.\n",
    "\n",
    "2. Feature Engineering in Data Generation:\n",
    "Features were normalized before creating the target\n",
    "The function used linear combinations with weights\n",
    "Interaction terms were simple multiplicative relationships\n",
    "This structure favors linear models like Logistic Regression\n",
    "\n",
    "3. Dataset Characteristics:\n",
    "\n",
    "Medium-sized dataset (1000 samples) may not provide enough complexity for Random Forest to excel\n",
    "Features have clear, interpretable relationships with the target\n",
    "Limited non-linear patterns in the underlying data generation process\n",
    "\n",
    "4. Random Forest Limitations:\n",
    "Random Forest excels with complex, non-linear relationships and feature interactions\n",
    "It might be experiencing some overfitting with the current dataset size\n",
    "The bootstrap sampling in Random Forest might be adding unnecessary noise to a relatively straightforward problem\n",
    "\n",
    "5. Preprocessing Impact:\n",
    "StandardScaler helps Logistic Regression handle different feature scales effectively\n",
    "Random Forest is scale-invariant, so it doesn't benefit as much from preprocessing\n",
    "The preprocessing pipeline may be more optimized for linear models\n",
    "\n",
    "General Principle:\n",
    "While Random Forest is often more powerful for complex, real-world datasets with non-linear relationships, simpler linear models can outperform when the underlying relationships are indeed linear or when the dataset size is limited. This demonstrates the importance of understanding your data and trying multiple approaches rather than assuming more complex models are always better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
